{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>perturbation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAACATACACGTAC_dc3hLPS_A8</td>\n",
       "      <td>m_Rel_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAACATACATGTCG_dc3hLPS_A8</td>\n",
       "      <td>m_Nfkb1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAACATACCAACTG_dc3hLPS_A8</td>\n",
       "      <td>m_Egr2_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAACATACTCCTTA_dc3hLPS_A8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAACATACTCTCCG_dc3hLPS_A8</td>\n",
       "      <td>m_Runx1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32772</th>\n",
       "      <td>32772</td>\n",
       "      <td>TTTCTACTGACGAG_dc3hLPS_D9</td>\n",
       "      <td>m_Stat2_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32773</th>\n",
       "      <td>32773</td>\n",
       "      <td>TTTCTACTTCTTAC_dc3hLPS_D9</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32774</th>\n",
       "      <td>32774</td>\n",
       "      <td>TTTGACTGAAAAGC_dc3hLPS_D9</td>\n",
       "      <td>m_Rela_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32775</th>\n",
       "      <td>32775</td>\n",
       "      <td>TTTGCATGCGACAT_dc3hLPS_D9</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32776</th>\n",
       "      <td>32776</td>\n",
       "      <td>TTTGCATGCTGAGT_dc3hLPS_D9</td>\n",
       "      <td>m_MouseNTC_100_A_67005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32777 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                          0            perturbation\n",
       "0               0  AAACATACACGTAC_dc3hLPS_A8                 m_Rel_3\n",
       "1               1  AAACATACATGTCG_dc3hLPS_A8               m_Nfkb1_2\n",
       "2               2  AAACATACCAACTG_dc3hLPS_A8                m_Egr2_4\n",
       "3               3  AAACATACTCCTTA_dc3hLPS_A8                 control\n",
       "4               4  AAACATACTCTCCG_dc3hLPS_A8               m_Runx1_2\n",
       "...           ...                        ...                     ...\n",
       "32772       32772  TTTCTACTGACGAG_dc3hLPS_D9               m_Stat2_4\n",
       "32773       32773  TTTCTACTTCTTAC_dc3hLPS_D9                 control\n",
       "32774       32774  TTTGACTGAAAAGC_dc3hLPS_D9                m_Rela_1\n",
       "32775       32775  TTTGCATGCGACAT_dc3hLPS_D9                 control\n",
       "32776       32776  TTTGCATGCTGAGT_dc3hLPS_D9  m_MouseNTC_100_A_67005\n",
       "\n",
       "[32777 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "import scanpy as sc\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:\\Users\\W\\Desktop\\Assignments\\Data Science\\project\\repreduce 1\\data_visual\\adata_1\"\n",
    "adata = sc.read_h5ad(path)\n",
    "adata.obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m_Rel_3', 'm_Nfkb1_2', 'm_Egr2_4', 'control', 'm_Runx1_2', ..., 'm_Egr1_4', 'm_Hif1a_1', 'm_Irf1_1', 'm_Cebpb_1', 'm_Irf2_2']\n",
      "Length: 58\n",
      "Categories (58, object): ['control', 'm_Ahr_1', 'm_Ahr_3', 'm_Atf3_1', ..., 'm_Stat2_2', 'm_Stat2_3', 'm_Stat2_4', 'm_Stat3_3']\n",
      "       Unnamed: 0                            0 gene_symbol   symbols\n",
      "4976         4976  ENSMUSG00000028988_Ctnnbip1    Ctnnbip1  Ctnnbip1\n",
      "7880         7880     ENSMUSG00000030505_Prmt3       Prmt3     Prmt3\n",
      "9003         9003    ENSMUSG00000020260_Pofut2      Pofut2    Pofut2\n",
      "8428         8428    ENSMUSG00000042289_Hsd3b7      Hsd3b7    Hsd3b7\n",
      "1128         1128   ENSMUSG00000104206_Gm32250     Gm32250   Gm32250\n",
      "...           ...                          ...         ...       ...\n",
      "14321       14321     ENSMUSG00000021109_Hif1a       Hif1a     Hif1a\n",
      "14986       14986      ENSMUSG00000042622_Maff        Maff      Maff\n",
      "15812       15812     ENSMUSG00000022952_Runx1       Runx1     Runx1\n",
      "16882       16882      ENSMUSG00000038418_Egr1        Egr1      Egr1\n",
      "17251       17251      ENSMUSG00000024927_Rela        Rela      Rela\n",
      "\n",
      "[5018 rows x 4 columns]\n",
      "4976    Ctnnbip1\n",
      "7880       Prmt3\n",
      "9003      Pofut2\n",
      "8428      Hsd3b7\n",
      "1128     Gm32250\n",
      "Name: gene_symbol, dtype: object\n",
      "✅ Cleaned perturbed genes: ['Rel', 'Nfkb1', 'Egr2', 'Runx1', 'Maff', 'Rel', 'Stat2', 'Hif1a', 'Maff', 'Stat1', 'Rela', 'Stat1', 'E2f4', 'Stat2', 'Irf4', 'Stat2', 'Irf2', 'Relb', 'Stat1', 'Stat3', 'Rel', 'Spi1', 'Ctcf', 'Rela', 'Nfkb1', 'E2f1', 'Irf2', 'Cebpb', 'Nfkb1', 'Spi1', 'Irf1', 'E2f1', 'Egr2', 'Runx1', 'Spi1', 'MouseNTC', 'Irf2', 'Irf1', 'Irf4', 'Ahr', 'Rela', 'Junb', 'Atf3', 'Hif1a', 'Ets2', 'Ahr', 'E2f4', 'Irf4', 'Ets2', 'Atf3', 'Ctcf', 'E2f4', 'Egr1', 'Hif1a', 'Irf1', 'Cebpb', 'Irf2']\n",
      "✅ Matched genes in adata.var: ['Rel', 'Nfkb1', 'Egr2', 'Runx1', 'Maff', 'Rel', 'Stat2', 'Hif1a', 'Maff', 'Stat1', 'Rela', 'Stat1', 'E2f4', 'Stat2', 'Irf4', 'Stat2', 'Irf2', 'Relb', 'Stat1', 'Stat3', 'Rel', 'Spi1', 'Ctcf', 'Rela', 'Nfkb1', 'E2f1', 'Irf2', 'Cebpb', 'Nfkb1', 'Spi1', 'Irf1', 'E2f1', 'Egr2', 'Runx1', 'Spi1', 'Irf2', 'Irf1', 'Irf4', 'Ahr', 'Rela', 'Junb', 'Atf3', 'Hif1a', 'Ets2', 'Ahr', 'E2f4', 'Irf4', 'Ets2', 'Atf3', 'Ctcf', 'E2f4', 'Egr1', 'Hif1a', 'Irf1', 'Cebpb', 'Irf2']\n"
     ]
    }
   ],
   "source": [
    "perturbation_names = adata.obs[\"perturbation\"].unique()\n",
    "print(perturbation_names)\n",
    "print(adata.var)\n",
    "import pandas as pd\n",
    "\n",
    "perturbation_names = [gene for gene in perturbation_names if gene != \"control\"]\n",
    "\n",
    "# Extract gene symbols from adata.var (remove ENSEMBL IDs)\n",
    "adata.var[\"gene_symbol\"] = adata.var[\"0\"].str.split(\"_\").str[-1]  # Extract last part after \"_\"\n",
    "\n",
    "# Display example\n",
    "print(adata.var[\"gene_symbol\"].head())\n",
    "\n",
    "# Example perturbed genes (from experimental data)\n",
    "\n",
    "\n",
    "# Clean perturbed gene names by removing prefixes (`m_`) and suffixes (`_3`, `_2`, etc.)\n",
    "cleaned_perturbed_genes = [gene.split(\"_\")[1] for gene in perturbation_names]\n",
    "\n",
    "print(\"✅ Cleaned perturbed genes:\", cleaned_perturbed_genes)\n",
    "\n",
    "# Check if each perturbed gene exists in the extracted gene symbols\n",
    "matched_genes = [gene for gene in cleaned_perturbed_genes if gene in adata.var[\"gene_symbol\"].values]\n",
    "\n",
    "print(\"✅ Matched genes in adata.var:\", matched_genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Genes: {'Rel': 'Rel', 'Nfkb1': 'Nfkb1', 'Egr2': 'Egr2', 'Runx1': 'Runx1', 'Maff': 'Maff', 'Stat2': 'Stat2', 'Hif1a': 'Hif1a', 'Stat1': 'Stat1', 'Rela': 'Rela', 'E2f4': 'E2f4', 'Irf4': 'Irf4', 'Irf2': 'Irf2', 'Relb': 'Relb', 'Stat3': 'Stat3', 'Spi1': 'Spi1', 'Ctcf': 'Ctcf', 'E2f1': 'E2f1', 'Cebpb': 'Cebpb', 'Irf1': 'Irf1', 'Ahr': 'Ahr', 'Junb': 'Junb', 'Atf3': 'Atf3', 'Ets2': 'Ets2', 'Egr1': 'Egr1'}\n",
      "Recovered 24 perturbation genes.\n",
      "       Unnamed: 0                            0 gene_symbol   symbols\n",
      "4976         4976  ENSMUSG00000028988_Ctnnbip1    Ctnnbip1  Ctnnbip1\n",
      "7880         7880     ENSMUSG00000030505_Prmt3       Prmt3     Prmt3\n",
      "9003         9003    ENSMUSG00000020260_Pofut2      Pofut2    Pofut2\n",
      "8428         8428    ENSMUSG00000042289_Hsd3b7      Hsd3b7    Hsd3b7\n",
      "1128         1128   ENSMUSG00000104206_Gm32250     Gm32250   Gm32250\n",
      "...           ...                          ...         ...       ...\n",
      "14321       14321     ENSMUSG00000021109_Hif1a       Hif1a     Hif1a\n",
      "14986       14986      ENSMUSG00000042622_Maff        Maff      Maff\n",
      "15812       15812     ENSMUSG00000022952_Runx1       Runx1     Runx1\n",
      "16882       16882      ENSMUSG00000038418_Egr1        Egr1      Egr1\n",
      "17251       17251      ENSMUSG00000024927_Rela        Rela      Rela\n",
      "\n",
      "[5018 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def sketch_perturbation_genes(adata, perturbation_names):\n",
    "    \n",
    "    # Sketch the symbols in original AnnData gene names\n",
    "    adata.var[\"symbols\"] = adata.var[\"0\"].str.split(\"_\").str[-1]\n",
    "\n",
    "    # Find the cleaned perturbed names \"m_Rel_1\": \"Rel\"\n",
    "    cleaned_perturbation_genes = [gene.split(\"_\")[1] for gene in perturbation_names if gene != \"control\"]\n",
    "\n",
    "    # Match the cleaned perturbed names with symbols in original AnnData\n",
    "    matched_genes = {}\n",
    "    for gene in cleaned_perturbation_genes:\n",
    "        match, score = process.extractOne(gene, adata.var[\"symbols\"].values)\n",
    "        \n",
    "        # Find the best matches\n",
    "        if score > 85 and gene.lower() in match.lower():\n",
    "            matched_genes[gene] = match\n",
    "\n",
    "    print(\"Matched Genes:\", matched_genes)\n",
    "\n",
    "    # Sketch the indices for the removed genes in the original AnnData\n",
    "    remove_indices = adata.var.index[adata.var[\"symbols\"].isin(matched_genes.values())].tolist()\n",
    "    remove_indices = list(set(remove_indices))\n",
    "    if not adata.var.index.is_unique:\n",
    "        print(\"Warning: `adata.var.index` contains duplicate values!\")\n",
    "        adata.var = adata.var[~adata.var.index.duplicated(keep=\"first\")]  # Keep only first occurrence\n",
    "    # Extract the removed X and var in the AnnData for the missing genes\n",
    "    adding_X = adata[:, remove_indices].X\n",
    "    adding_var = adata.var.loc[remove_indices]\n",
    "\n",
    "    # Update the AnnData with missing genes\n",
    "    perturbation_adata = sc.AnnData(X=adding_X, obs=adata.obs.copy(), var=adding_var.copy())\n",
    "\n",
    "    print(f\"Recovered {len(remove_indices)} perturbation genes.\")\n",
    "    \n",
    "    return perturbation_adata \n",
    "\n",
    "preturbed_adata = sketch_perturbation_genes(adata, perturbation_names)\n",
    "print(adata.var)\n",
    "preturbed_adata.var\n",
    "perturbation_symbols = preturbed_adata.var[\"symbols\"]\n",
    "perturbation_index = preturbed_adata.var.index\n",
    "\n",
    "gene_symbols = adata.var[\"symbols\"]\n",
    "gene_index = adata.var.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>perturbation</th>\n",
       "      <th>symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAACATACACGTAC_dc3hLPS_A8</td>\n",
       "      <td>m_Rel_3</td>\n",
       "      <td>Rel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAACATACATGTCG_dc3hLPS_A8</td>\n",
       "      <td>m_Nfkb1_2</td>\n",
       "      <td>Nfkb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAACATACCAACTG_dc3hLPS_A8</td>\n",
       "      <td>m_Egr2_4</td>\n",
       "      <td>Egr2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAACATACTCCTTA_dc3hLPS_A8</td>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAACATACTCTCCG_dc3hLPS_A8</td>\n",
       "      <td>m_Runx1_2</td>\n",
       "      <td>Runx1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32772</th>\n",
       "      <td>32772</td>\n",
       "      <td>TTTCTACTGACGAG_dc3hLPS_D9</td>\n",
       "      <td>m_Stat2_4</td>\n",
       "      <td>Stat2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32773</th>\n",
       "      <td>32773</td>\n",
       "      <td>TTTCTACTTCTTAC_dc3hLPS_D9</td>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32774</th>\n",
       "      <td>32774</td>\n",
       "      <td>TTTGACTGAAAAGC_dc3hLPS_D9</td>\n",
       "      <td>m_Rela_1</td>\n",
       "      <td>Rela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32775</th>\n",
       "      <td>32775</td>\n",
       "      <td>TTTGCATGCGACAT_dc3hLPS_D9</td>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32776</th>\n",
       "      <td>32776</td>\n",
       "      <td>TTTGCATGCTGAGT_dc3hLPS_D9</td>\n",
       "      <td>m_MouseNTC_100_A_67005</td>\n",
       "      <td>MouseNTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32777 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                          0            perturbation   symbols\n",
       "0               0  AAACATACACGTAC_dc3hLPS_A8                 m_Rel_3       Rel\n",
       "1               1  AAACATACATGTCG_dc3hLPS_A8               m_Nfkb1_2     Nfkb1\n",
       "2               2  AAACATACCAACTG_dc3hLPS_A8                m_Egr2_4      Egr2\n",
       "3               3  AAACATACTCCTTA_dc3hLPS_A8                 control       NaN\n",
       "4               4  AAACATACTCTCCG_dc3hLPS_A8               m_Runx1_2     Runx1\n",
       "...           ...                        ...                     ...       ...\n",
       "32772       32772  TTTCTACTGACGAG_dc3hLPS_D9               m_Stat2_4     Stat2\n",
       "32773       32773  TTTCTACTTCTTAC_dc3hLPS_D9                 control       NaN\n",
       "32774       32774  TTTGACTGAAAAGC_dc3hLPS_D9                m_Rela_1      Rela\n",
       "32775       32775  TTTGCATGCGACAT_dc3hLPS_D9                 control       NaN\n",
       "32776       32776  TTTGCATGCTGAGT_dc3hLPS_D9  m_MouseNTC_100_A_67005  MouseNTC\n",
       "\n",
       "[32777 rows x 4 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated gene embeddings with shape (24, 64).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import scanpy as sc\n",
    "\n",
    "def compute_grn_from_covariance(adata, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Computes a Gene Regulatory Network (GRN) using gene expression covariance.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing gene expression data.\n",
    "    - threshold: Minimum absolute covariance value for an edge.\n",
    "\n",
    "    Returns:\n",
    "    - edge_index: PyTorch tensor representing the GRN edges.\n",
    "    \"\"\"\n",
    "    expression_matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "    covariance_matrix = np.cov(expression_matrix, rowvar=False)\n",
    "\n",
    "    edge_list = []\n",
    "    num_genes = covariance_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_genes):\n",
    "        for j in range(i + 1, num_genes):\n",
    "            if abs(covariance_matrix[i, j]) > threshold:\n",
    "                edge_list.append((i, j))\n",
    "                edge_list.append((j, i))  # Undirected graph\n",
    "\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).T\n",
    "    return edge_index\n",
    "\n",
    "# Compute GRN edges\n",
    "grn_edge_index = compute_grn_from_covariance(preturbed_adata)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GeneEmbeddingGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GeneEmbeddingGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize random input features (gene embeddings)\n",
    "num_genes = preturbed_adata.shape[1]\n",
    "embedding_dim = 64  # Gene embedding dimension\n",
    "gene_features = torch.randn(num_genes, embedding_dim)\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "gene_graph = Data(x=gene_features, edge_index=grn_edge_index)\n",
    "\n",
    "# Instantiate and run GNN\n",
    "gene_gnn = GeneEmbeddingGNN(input_dim=embedding_dim, hidden_dim=128, output_dim=embedding_dim)\n",
    "perturbation_embeddings = gene_gnn(gene_graph.x, gene_graph.edge_index)\n",
    "\n",
    "# Convert to DataFrame\n",
    "perturbation_embeddings_df = pd.DataFrame(perturbation_embeddings.detach().numpy(), index=preturbed_adata.var_names)\n",
    "\n",
    "print(f\" Generated gene embeddings with shape {perturbation_embeddings_df.shape}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5619,  0.5695, -0.4530,  ..., -0.1940,  0.1096, -0.2489],\n",
       "        [ 0.3643, -0.4883,  0.4908,  ...,  0.3322,  0.1626,  0.0056],\n",
       "        [ 0.4872,  0.1893,  0.2931,  ..., -0.2984, -0.1759,  0.7876],\n",
       "        ...,\n",
       "        [ 0.4763,  0.7349, -0.9968,  ..., -0.0011, -0.3146, -0.1546],\n",
       "        [ 0.5294, -0.2087, -0.1362,  ...,  0.5635, -0.5476,  0.0874],\n",
       "        [ 1.0526,  1.1167, -1.0149,  ...,  0.1186, -0.4439, -0.2328]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbation_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated gene embeddings with shape (5018, 64).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import scanpy as sc\n",
    "\n",
    "def compute_grn_from_covariance(adata, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Computes a Gene Regulatory Network (GRN) using gene expression covariance.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing gene expression data.\n",
    "    - threshold: Minimum absolute covariance value for an edge.\n",
    "\n",
    "    Returns:\n",
    "    - edge_index: PyTorch tensor representing the GRN edges.\n",
    "    \"\"\"\n",
    "    expression_matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "    covariance_matrix = np.cov(expression_matrix, rowvar=False)\n",
    "\n",
    "    edge_list = []\n",
    "    num_genes = covariance_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_genes):\n",
    "        for j in range(i + 1, num_genes):\n",
    "            if abs(covariance_matrix[i, j]) > threshold:\n",
    "                edge_list.append((i, j))\n",
    "                edge_list.append((j, i))  # Undirected graph\n",
    "\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).T\n",
    "    return edge_index\n",
    "\n",
    "# Compute GRN edges\n",
    "grn_edge_index = compute_grn_from_covariance(adata)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GeneEmbeddingGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GeneEmbeddingGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize random input features (gene embeddings)\n",
    "num_genes = adata.shape[1]\n",
    "embedding_dim = 64  # Gene embedding dimension\n",
    "gene_features = torch.randn(num_genes, embedding_dim)\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "gene_graph = Data(x=gene_features, edge_index=grn_edge_index)\n",
    "\n",
    "# Instantiate and run GNN\n",
    "gene_gnn = GeneEmbeddingGNN(input_dim=embedding_dim, hidden_dim=128, output_dim=embedding_dim)\n",
    "gene_embeddings = gene_gnn(gene_graph.x, gene_graph.edge_index)\n",
    "\n",
    "# Convert to DataFrame\n",
    "gene_embeddings_df = pd.DataFrame(gene_embeddings.detach().numpy(), index=adata.var_names)\n",
    "\n",
    "print(f\"✅ Generated gene embeddings with shape {gene_embeddings_df.shape}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4901,  0.3833,  0.3979,  ..., -0.9739, -0.8685, -0.1473],\n",
       "        [ 0.0157,  0.0458,  0.8190,  ..., -0.5170,  1.1289, -0.6276],\n",
       "        [ 1.2202,  0.8001, -0.7026,  ..., -0.2977, -2.5305, -0.3518],\n",
       "        ...,\n",
       "        [ 0.0939,  1.3783,  0.2176,  ..., -0.8683, -0.6194, -0.9995],\n",
       "        [ 0.3055,  0.7999,  0.2410,  ..., -0.0323, -0.7287, -0.2633],\n",
       "        [-0.3389,  0.8239, -0.6588,  ..., -0.1849,  0.8136, -0.3119]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 24 positions of perturbation genes in gene_index.\n",
      "Perturbation Positions: [5004, 5003, 5009, 5015, 5011, 3155, 2113, 707, 5000, 5007, 5016, 4498, 5012, 5005, 5006, 1800, 5013, 5010, 5014, 2025, 5017, 5001, 5002, 5008]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-5.5604e-04,  7.5179e-02, -4.9767e-03,  ..., -4.5681e-02,\n",
       "         -3.9258e-04, -3.9346e-02],\n",
       "        [-2.2259e-01,  1.7317e-01, -5.5421e-02,  ..., -7.8946e-01,\n",
       "         -1.3193e+00, -2.9324e-01],\n",
       "        [-1.7038e-02,  7.3041e-02,  2.4959e-02,  ..., -3.0534e-02,\n",
       "         -1.2774e-02, -2.2900e-02],\n",
       "        ...,\n",
       "        [ 3.5478e-03,  8.4826e-02,  4.4246e-02,  ..., -7.9988e-02,\n",
       "          8.9703e-03, -7.2632e-02],\n",
       "        [ 1.5704e-02,  4.7382e-02, -1.8966e-02,  ..., -5.6767e-02,\n",
       "         -6.3808e-02, -4.8464e-02],\n",
       "        [-3.3164e-01,  1.1002e+00, -8.7090e-02,  ..., -3.5304e-01,\n",
       "         -7.8286e-01, -2.2205e-01]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store positions of perturbation genes in gene_index\n",
    "perturbation_positions = []\n",
    "\n",
    "# Iterate through perturbation indices and find matching positions in gene_index\n",
    "for perturbation_id in perturbation_index:\n",
    "    for pos, gene_id in enumerate(gene_index):\n",
    "        if gene_id == perturbation_id:  # Match found\n",
    "            perturbation_positions.append(pos)  # Store the position\n",
    "            break  # Stop after finding the first match to maintain order\n",
    "\n",
    "print(f\"✅ Extracted {len(perturbation_positions)} positions of perturbation genes in gene_index.\")\n",
    "print(f\"Perturbation Positions: {perturbation_positions}\")\n",
    "\n",
    "extracted_gene_embeddings = gene_embeddings[perturbation_positions,:]\n",
    "extracted_gene_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CompositionalModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines perturbation embeddings with gene embeddings to model interactions.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CompositionalModule, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim * 2, embedding_dim)  # Compress combined embeddings\n",
    "\n",
    "    def forward(self, perturbation_embedding, extracted_gene_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass for compositional module:\n",
    "        - Combines perturbation and gene embeddings.\n",
    "        - Applies transformation to capture relationships.\n",
    "\n",
    "        Inputs:\n",
    "        - perturbation_embedding: Tensor of shape (batch_size, embedding_dim)\n",
    "        - extracted_gene_embedding: Tensor of shape (batch_size, embedding_dim)\n",
    "\n",
    "        Output:\n",
    "        - Tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        combined = torch.cat([perturbation_embedding, extracted_gene_embedding], dim=1)  # Concatenate embeddings\n",
    "        print(f\"🔹 Perturbation embedding shape: {perturbation_embedding.shape}\")  # Should be (batch, 64)\n",
    "        print(f\"🔹 Extracted gene embedding shape: {extracted_gene_embedding.shape}\")  # Should be (batch, 64)\n",
    "        \n",
    "        return self.fc(combined)  # Apply a linear transformation\n",
    "\n",
    "\n",
    "class CrossGeneDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts post-perturbation gene expression based on embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_genes):\n",
    "        super(CrossGeneDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_genes)  # Maps embedding to gene expression space\n",
    "\n",
    "    def forward(self, combined_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass for cross-gene decoder:\n",
    "        - Maps combined embedding to gene expression space.\n",
    "\n",
    "        Input:\n",
    "        - combined_embedding: Tensor of shape (batch_size, embedding_dim)\n",
    "\n",
    "        Output:\n",
    "        - Tensor of shape (batch_size, num_genes) (Predicted expression)\n",
    "        \"\"\"\n",
    "        return self.fc(combined_embedding)  # Outputs predicted expression\n",
    "\n",
    "\n",
    "class AutofocusLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Autofocus direction-aware loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2, lambda_reg=0.1):\n",
    "        super(AutofocusLoss, self).__init__()\n",
    "        self.gamma = gamma  # Weight for autofocus loss\n",
    "        self.lambda_reg = lambda_reg  # Regularization term\n",
    "        self.mse = nn.MSELoss()  # Standard loss for expression prediction\n",
    "\n",
    "    def forward(self, predicted_expression, true_expression, perturbation_embedding):\n",
    "        \"\"\"\n",
    "        Computes loss:\n",
    "        - MSE loss between predicted & true expression\n",
    "        - Autofocus loss penalizing large perturbations\n",
    "        - Regularization term for stable training\n",
    "\n",
    "        Inputs:\n",
    "        - predicted_expression: Tensor of shape (batch_size, num_genes)\n",
    "        - true_expression: Tensor of shape (batch_size, num_genes)\n",
    "        - perturbation_embedding: Tensor of shape (batch_size, embedding_dim)\n",
    "\n",
    "        Output:\n",
    "        - Scalar loss value\n",
    "        \"\"\"\n",
    "        mse_loss = self.mse(predicted_expression, true_expression)  # Ensure gradients flow\n",
    "        focus_loss = torch.mean(torch.abs(perturbation_embedding))  # Encourage stable perturbation embeddings\n",
    "        \n",
    "        # Total loss combines prediction error and embedding stability\n",
    "        total_loss = mse_loss + self.gamma * focus_loss + self.lambda_reg * torch.norm(perturbation_embedding, p=2)\n",
    "        \n",
    "        return total_loss  # No .detach(), maintain computational graph\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# def autofocus_loss(predicted_expression, true_expression, perturbation_names, gamma=2, lambda_reg=0.1, direction_lambda=1e-3):\n",
    "#     \"\"\"\n",
    "#     Adapted Autofocus Loss Function\n",
    "\n",
    "#     Args:\n",
    "#         predicted_expression (torch.Tensor): Model output for post-perturbation gene expression.\n",
    "#         true_expression (torch.Tensor): True post-perturbation gene expression.\n",
    "#         perturbation_names (list): List of perturbation conditions.\n",
    "#         gamma (float): Exponent for the autofocus loss.\n",
    "#         lambda_reg (float): Regularization term for uncertainty handling.\n",
    "#         direction_lambda (float): Weight for directional loss.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Computed loss value.\n",
    "#     \"\"\"\n",
    "#     perturbation_names = np.array(list(set(perturbation_names)))\n",
    "#     losses = torch.tensor(0.0, requires_grad=True).to(predicted_expression.device)\n",
    "#     print(len(perturbation_names), predicted_expression.size())\n",
    "#     for pert in set(perturbation_names):\n",
    "#         if pert != \"control\":\n",
    "#             pert_idx = np.where(perturbation_names == pert)[0]\n",
    "#             pred_pert = predicted_expression[pert_idx]\n",
    "#             true_pert = true_expression[pert_idx]\n",
    "\n",
    "#             # Compute standard MSE loss\n",
    "#             mse_loss = torch.mean((pred_pert - true_pert) ** (2 + gamma))\n",
    "\n",
    "#             # Autofocus regularization\n",
    "#             focus_loss = torch.mean(torch.abs(pred_pert))\n",
    "\n",
    "#             # Direction-aware regularization\n",
    "#             dir_loss = direction_lambda * torch.mean((torch.sign(true_pert) - torch.sign(pred_pert)) ** 2)\n",
    "\n",
    "#             # Combine all components\n",
    "#             loss = mse_loss + gamma * focus_loss + lambda_reg * dir_loss\n",
    "#             losses += loss\n",
    "\n",
    "#     return losses / len(set(perturbation_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Computed `true_expression_tensor` with shape: torch.Size([24, 5018])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Add a \"symbols\" column to adata.obs for matching\n",
    "adata.obs[\"symbols\"] = adata.obs[\"perturbation\"].str.split(\"_\").str[1]  # Extract gene names from perturbations\n",
    "\n",
    "# Initialize a list to store mean expression per perturbation\n",
    "perturbation_mean_expressions = []\n",
    "\n",
    "# Convert adata.X to dense array (if it's sparse) for easier computation\n",
    "expression_matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "\n",
    "# Step 2: Compute mean expression for each perturbation symbol\n",
    "for symbol in perturbation_symbols:\n",
    "    # Find indices of cells where perturbation matches this symbol\n",
    "    matching_cells_mask = adata.obs[\"symbols\"] == symbol\n",
    "\n",
    "    # Extract expression data for these cells\n",
    "    matching_expression = expression_matrix[matching_cells_mask]\n",
    "\n",
    "    # Compute mean expression across all matching cells (1 × 5024 vector)\n",
    "    mean_expression = matching_expression.mean(axis=0)\n",
    "\n",
    "    # Store the mean expression\n",
    "    perturbation_mean_expressions.append(mean_expression)\n",
    "\n",
    "# Convert to tensor for training\n",
    "true_expression = torch.tensor(perturbation_mean_expressions, dtype=torch.float32)\n",
    "\n",
    "# Display final shape\n",
    "print(f\"✅ Computed `true_expression_tensor` with shape: {true_expression.shape}\")  # Expected: (24, 5024) or similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Perturbation embedding shape: torch.Size([24, 64])\n",
      "🔹 Extracted gene embedding shape: torch.Size([24, 64])\n",
      "Predicted Expression requires grad: True\n",
      "Perturbation Embedding requires grad: True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "true_expression shape: torch.Size([24, 64])\n",
      "true_expression requires_grad: False\n",
      "true_expression values (first 5 rows):\n",
      " tensor([[0.3439, 0.0469, 0.5488,  ..., 0.2109, 0.2470, 0.7128],\n",
      "        [0.3190, 0.0383, 0.5597,  ..., 0.2111, 0.2634, 0.6444],\n",
      "        [0.3317, 0.0233, 0.5922,  ..., 0.2264, 0.2855, 0.7318],\n",
      "        [0.3090, 0.0508, 0.5396,  ..., 0.2265, 0.2490, 0.7076],\n",
      "        [0.3064, 0.0327, 0.5491,  ..., 0.1894, 0.2616, 0.6917]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_expression values (first 5 rows):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, true_expression[:\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m     50\u001b[0m  \u001b[38;5;66;03m# Should be True\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\nlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\nlp\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "gamma = 2\n",
    "lambda_reg = 0.1\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 200\n",
    "\n",
    "# Initialize Model Components\n",
    "compositional_module = CompositionalModule(embedding_dim)\n",
    "cross_gene_decoder = CrossGeneDecoder(embedding_dim, num_genes=adata.shape[1])\n",
    "loss_fn = AutofocusLoss(gamma, lambda_reg)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(compositional_module.parameters()) + list(cross_gene_decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "\n",
    "# Ensure perturbation embeddings require gradients\n",
    "perturbation_embeddings.requires_grad_(True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    # optimizer.zero_grad()  # Reset gradients before backprop\n",
    "\n",
    "    # Sample perturbation and gene embeddings\n",
    "    perturb_embed = perturbation_embeddings\n",
    "    gene_embed = extracted_gene_embeddings\n",
    "\n",
    "    # Combine embeddings\n",
    "    combined_embedding = compositional_module(perturb_embed, gene_embed)\n",
    "\n",
    "    # Predict post-perturbation expression\n",
    "    predicted_expression = cross_gene_decoder(combined_embedding)\n",
    "    # print(predicted_expression, true_expression, perturb_embed)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(predicted_expression, true_expression, perturb_embed)\n",
    "\n",
    "    # Debug prints\n",
    "    print(f\"Predicted Expression requires grad: {predicted_expression.requires_grad}\")\n",
    "    print(f\"Perturbation Embedding requires grad: {perturb_embed.requires_grad}\")\n",
    "\n",
    "    print(predicted_expression.requires_grad)  # Must be True\n",
    "    print(combined_embedding.requires_grad)  # Must be True\n",
    "    print(perturb_embed.requires_grad)  # Must be True\n",
    "    print(gene_embed.requires_grad)  # Must be True\n",
    "    \n",
    "    print(\"true_expression shape:\", perturb_embed.shape)\n",
    "    print(\"true_expression requires_grad:\", true_expression.requires_grad)  # MUST be False\n",
    "    print(\"true_expression values (first 5 rows):\\n\", true_expression[:5])\n",
    "\n",
    "     # Should be True\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True) tensor([[-0.2432, -0.7535,  0.2278],\n",
      "        [-0.7002,  0.3161,  1.3174],\n",
      "        [ 1.0295, -1.2459,  0.2643]], requires_grad=True)\n",
      "Perturb Embed Shape: torch.Size([3, 3])\n",
      "Gene Embed Shape: torch.Size([3, 3])\n",
      "True Expression Shape: torch.Size([3, 3])\n",
      "Predicted Expression Shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Small batch size (3 examples instead of 24 for testing)\n",
    "batch_size = 3\n",
    "embedding_dim = 3\n",
    "num_genes = 3  # Total genes\n",
    "\n",
    "# Simulated embeddings\n",
    "perturb_embed = torch.randn(batch_size, embedding_dim, requires_grad=True)  # ✅ Needs grad\n",
    "extracted_gene_embed = torch.randn(batch_size, embedding_dim, requires_grad=True)  # ✅ Needs grad\n",
    "\n",
    "# Simulated expression matrices\n",
    "true_expression = torch.randn(batch_size, num_genes)  # ❌ No need for gradient\n",
    "predicted_expression = torch.randn(batch_size, num_genes, requires_grad=True)  # ✅ Needs grad\n",
    "print(perturb_embed, gene_embed)\n",
    "# Print shapes\n",
    "print(f\"Perturb Embed Shape: {perturb_embed.shape}\")\n",
    "print(f\"Gene Embed Shape: {extracted_gene_embed.shape}\")\n",
    "print(f\"True Expression Shape: {true_expression.shape}\")\n",
    "print(f\"Predicted Expression Shape: {predicted_expression.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True) tensor([[-0.2432, -0.7535,  0.2278],\n",
      "        [-0.7002,  0.3161,  1.3174],\n",
      "        [ 1.0295, -1.2459,  0.2643]], requires_grad=True)\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True) tensor([[-0.2432, -0.7535,  0.2278],\n",
      "        [-0.7002,  0.3161,  1.3174],\n",
      "        [ 1.0295, -1.2459,  0.2643]], requires_grad=True)\n",
      "Combined Embedding Shape: torch.Size([3, 3])\n",
      "Predicted Expression Shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "class CompositionalModule(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CompositionalModule, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim * 2, embedding_dim)  \n",
    "\n",
    "    def forward(self, perturbation_embedding, extracted_gene_embedding):\n",
    "        combined = torch.cat([perturbation_embedding, extracted_gene_embedding], dim=1)  \n",
    "        output = self.fc(combined)  \n",
    "        return output  \n",
    "\n",
    "class CrossGeneDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_genes):\n",
    "        super(CrossGeneDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_genes)  \n",
    "\n",
    "    def forward(self, combined_embedding):\n",
    "        return self.fc(combined_embedding)  \n",
    "\n",
    "# Initialize modules\n",
    "compositional_module = CompositionalModule(embedding_dim)\n",
    "cross_gene_decoder = CrossGeneDecoder(embedding_dim, num_genes)\n",
    "print(perturb_embed, gene_embed)\n",
    "# Forward pass\n",
    "combined_embedding = compositional_module(perturb_embed, extracted_gene_embed)\n",
    "predicted_expression = cross_gene_decoder(combined_embedding)\n",
    "print(perturb_embed, gene_embed)\n",
    "print(f\"Combined Embedding Shape: {combined_embedding.shape}\")  # Should be (3, 64)\n",
    "print(f\"Predicted Expression Shape: {predicted_expression.shape}\")  # Should be (3, 5024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss at start: 1.0126\n",
      "Loss requires grad? True\n",
      "Predicted Expression requires grad? True\n",
      "Perturbation Embedding requires grad? True\n",
      "Epoch 0: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 10: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 20: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 30: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 40: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 50: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 60: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 70: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 80: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 90: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 100: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 110: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 120: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 130: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 140: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 150: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 160: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 170: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 180: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "Epoch 190: Loss = 1.0126\n",
      "tensor([[ 0.5125, -1.0001, -0.0074],\n",
      "        [-1.4243,  0.0031,  1.6289],\n",
      "        [ 0.6273,  0.2213, -1.7466]], requires_grad=True)\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "class AutofocusLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, lambda_reg=0.1):\n",
    "        super(AutofocusLoss, self).__init__()\n",
    "        self.gamma = gamma  \n",
    "        self.lambda_reg = lambda_reg  \n",
    "        self.mse = nn.MSELoss()  \n",
    "\n",
    "    def forward(self, predicted_expression, true_expression, perturb_embed):\n",
    "        mse_loss = self.mse(predicted_expression, true_expression)  # ✅ Shape (3, 5024)\n",
    "        reg_loss = self.lambda_reg * torch.norm(perturb_embed, p=2)  # ✅ Regularization on (3, 64)\n",
    "        total_loss = mse_loss + reg_loss\n",
    "        return total_loss  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ensure model is in training mode\n",
    "compositional_module.train()\n",
    "cross_gene_decoder.train()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients before each iteration\n",
    "\n",
    "    # Forward pass\n",
    "    perturb_embed = perturb_embed  # From Perturbation GNN\n",
    "    gene_embed = extracted_gene_embed  # From Gene GNN\n",
    "    # print(perturb_embed, gene_embed)\n",
    "    combined_embedding = compositional_module(perturb_embed, gene_embed)\n",
    "    predicted_expression = cross_gene_decoder(combined_embedding)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(predicted_expression, true_expression, perturb_embed)\n",
    "\n",
    "    # Debugging Checks\n",
    "    if epoch == 0:  # Print checks only for the first iteration\n",
    "        print(f\"✅ Loss at start: {loss.item():.4f}\")\n",
    "        print(f\"Loss requires grad? {loss.requires_grad}\")\n",
    "        print(f\"Predicted Expression requires grad? {predicted_expression.requires_grad}\")\n",
    "        print(f\"Perturbation Embedding requires grad? {perturb_embed.requires_grad}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward(retain_graph=False)  # Only use retain_graph=True if needed\n",
    "\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "        print(perturb_embed)\n",
    "# Ensure models are in evaluation mode after training\n",
    "compositional_module.eval()\n",
    "cross_gene_decoder.eval()\n",
    "print(\"✅ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\W\\\\Desktop\\\\Assignments\\\\Data Science\\\\project\\\\repreduce 1\\\\data_visual\\\\adata_1\\\\gene2go_all.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgears\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PertData, GEARS\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# get data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pert_data \u001b[38;5;241m=\u001b[39m \u001b[43mPertData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAssignments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData Science\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mrepreduce 1\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata_visual\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43madata_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# load dataset in paper: norman, adamson, dixit.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pert_data\u001b[38;5;241m.\u001b[39mload(data_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\nlp\\Lib\\site-packages\\gears\\pertdata.py:93\u001b[0m, in \u001b[0;36mPertData.__init__\u001b[1;34m(self, data_path, gene_set_path, default_pert_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[0;32m     92\u001b[0m server_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://dataverse.harvard.edu/api/access/datafile/6153417\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 93\u001b[0m \u001b[43mdataverse_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgene2go_all.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene2go_all.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgene2go \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\nlp\\Lib\\site-packages\\gears\\utils.py:68\u001b[0m, in \u001b[0;36mdataverse_download\u001b[1;34m(url, save_path)\u001b[0m\n\u001b[0;32m     66\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m     67\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_size_in_bytes, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(block_size):\n\u001b[0;32m     70\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\W\\\\Desktop\\\\Assignments\\\\Data Science\\\\project\\\\repreduce 1\\\\data_visual\\\\adata_1\\\\gene2go_all.pkl'"
     ]
    }
   ],
   "source": [
    "from gears import PertData, GEARS\n",
    "\n",
    "# get data\n",
    "pert_data = PertData(r\"C:\\Users\\W\\Desktop\\Assignments\\Data Science\\project\\repreduce 1\\data_visual\\adata_1\")\n",
    "# load dataset in paper: norman, adamson, dixit.\n",
    "pert_data.load(data_name = 'norman')\n",
    "# specify data split\n",
    "pert_data.prepare_split(split = 'simulation', seed = 1)\n",
    "# get dataloader with batch size\n",
    "pert_data.get_dataloader(batch_size = 32, test_batch_size = 128)\n",
    "\n",
    "# set up and train a model\n",
    "gears_model = GEARS(pert_data, device = 'cuda:8')\n",
    "gears_model.model_initialize(hidden_size = 64)\n",
    "gears_model.train(epochs = 20)\n",
    "\n",
    "# save/load model\n",
    "gears_model.save_model('gears')\n",
    "gears_model.load_pretrained('gears')\n",
    "\n",
    "# predict\n",
    "gears_model.predict([['CBL', 'CNN1'], ['FEV']])\n",
    "gears_model.GI_predict(['CBL', 'CNN1'], GI_genes_file=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
